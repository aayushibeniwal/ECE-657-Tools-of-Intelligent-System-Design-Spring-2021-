# import required packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

from tensorflow.keras.models import Sequential, load_model, save_model
from tensorflow.keras.layers import Dense, Dropout, LSTM, GRU, SimpleRNN
from pickle import dump, load
from sklearn.metrics import mean_squared_error as MSE


# YOUR IMPLEMENTATION
# Thoroughly comment your code to make it easy to follow

### create_dataset ###
# Function to create dataset depending on the number of days to look in past
# data = Dataframe from which new dataset is to be created
# n_past_days = Number of days to look in past while creating new dataset
# Returns newly created dataset with features labeled columns and Target values

def create_dataset(data, n_past_days):
    # Creating Dataset using n_past_days as a reference for going that many days in past

    # Create an empty features list for saving the features generated by using the past days values
    feature_list = []

    # j = 0 -> 4, As we have 4 unique features in the dataset Volume, Open, High, Low
    for j in range(0, 4):
        for i in range(1, n_past_days + 1):
            # i = identify the number of days in past
            if j == 0:
                feature_list.append("Volume_" + str(i))

            if j == 1:
                feature_list.append("Open_" + str(i))

            if j == 2:
                feature_list.append("High_" + str(i))

            if j == 3:
                feature_list.append("Low_" + str(i))

    # create an empty dataframe to store the created dataframe
    stocks_data = pd.DataFrame(0, index=np.arange(len(data) - (n_past_days - 1)), columns=['', ''])

    feature_idx = 0
    # Column indexer for fetching each column from main dataset
    for column_idx in range(2, 6):
        # iterate over number of past days to create new data value for each feature
        for current_day in range(0, n_past_days):

            day_indexer = (n_past_days - current_day) - 1
            temp_list = []

            # choose the values from a particular feature based on the numbers of days to look in past.
            for j in range(current_day, (len(data) - day_indexer)):
                temp_list.append(data.iloc[j, column_idx])
            stocks_data.insert(feature_idx, feature_list[feature_idx], temp_list, allow_duplicates=True)
            feature_idx += 1

    stocks_data = stocks_data.drop([len(data) - n_past_days])

    target_list = []
    # generate using next day's Open price
    for j in range(n_past_days, len(data)):
        target_list.append(data.iloc[j, 3])
    stocks_data.insert((n_past_days * 4), 'Target', target_list, allow_duplicates=True)
    stocks_data.drop(columns=['', ''], inplace=True)

    return stocks_data


if __name__ == "__main__":
    print("LOADING q2_dataset")
    # Load the main dataset provided
    stock_df = pd.read_csv('data/q2_dataset.csv')

    # Invert the csv file to go in reverse order as we need to pick the data from past days
    stock_df = stock_df.iloc[::-1]
    # print(stock_df)
    stocks = create_dataset(stock_df, n_past_days= 3)
    print("NEW dataset created with past days = 3\n")

    # Required commenting starts from here -------------------------------------------------------------------------
    '''
    # Train test split
    x = stocks.drop('Target', axis=1)
    y = stocks.loc[:,'Target':]

    # use sklearn train_test_split to randomize the data and split it into 70% train and 30% test data
    train_data, test_data, train_labels, test_labels = train_test_split(x, y, test_size=0.30, random_state=50)
    print("Size of train_data:   ", train_data.shape)
    print("Size of train_labels: ", train_labels.shape)
    print("Size of test_data:   ", test_data.shape)
    print("Size of test_labels: ", test_labels.shape)

    # Create Train and Test data csv files and save it locally in data folder
    # Concat train_data with train_labels
    train_data_RNN = pd.concat([train_data, train_labels], axis=1, join='inner')
    # Concat test_data with test_labels
    test_data_RNN = pd.concat([test_data, test_labels], axis=1, join='inner')

    # Save Train Data as train_data_RNN.csv
    train_data_RNN.to_csv('data/train_data_RNN.csv', index=False)
    # Save Test Data as test_data_RNN.csv
    test_data_RNN.to_csv('data/test_data_RNN.csv', index=False)

    # Load training data from train_data_RNN.csv
    train_dataset = pd.read_csv("data/train_data_RNN.csv")

    ## Carry our preprocessing and Prepare Data ##

    # used min max scalar to convert all the data from different features into a scale ranging from 0 to 1
    # Scaler for input features
    scaler = MinMaxScaler(feature_range=(0, 1))
    # Scaler for target
    scaler_label = MinMaxScaler(feature_range=(0, 1))

    # Fit the scaler on train data and train labels
    minMax_data = scaler.fit(train_dataset.iloc[:,:-1])
    minMax_label = scaler_label.fit(train_dataset.iloc[:,-1:])

    # save the scaler
    dump(scaler, open('data/minMax_data.pkl', 'wb'))
    dump(scaler_label, open('data/minMax_label.pkl', 'wb'))

    # Transform the train data and labels
    x_train = minMax_data.transform(train_dataset.iloc[:, :-1])
    y_train = minMax_label.transform(train_dataset.iloc[:, -1:])

    # Convert the scaled data back to numpy array to be useful while generating the model
    x_train = np.array(x_train)
    x_train_3D = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))

    # TRIED OUT VARIOUS APROACHES TO COMPUTE RNN USED THREE DIFFERENT MODELS

    # Commenting starts here ------------------------------------------------
    
    # #1st Model : LSTM
    # Recurrent Neural network using LSTM as a regressor
    LSTM_model = Sequential()
    # Layer 1
    LSTM_model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train_3D.shape[1], 1)))
    LSTM_model.add(Dropout(0.2))
    # Layer 2
    LSTM_model.add(LSTM(units=50, return_sequences=False))
    LSTM_model.add(Dropout(0.2))
    # Final Layer
    LSTM_model.add(Dense(units=1))
    # Compile LSTM model
    LSTM_model.compile(loss='mean_squared_error', optimizer='adam')
    LSTM_model.summary()
    # Fit the model to train the model using Train data
    print("RUNNING LSTM model on Training Data.....")
    LSTM_history = LSTM_model.fit(x_train_3D, y_train, epochs=150, batch_size=32)


    # #2nd model: Simple RNN
    # SimpleRNN model as a regressor
    simple_RNN_model = Sequential()
    # Layer  1
    simple_RNN_model.add(SimpleRNN(units=50, return_sequences=True, input_shape=(x_train_3D.shape[1], 1)))
    simple_RNN_model.add(Dropout(0.2))
    # Layer 2
    simple_RNN_model.add(SimpleRNN(units=50, return_sequences=False))
    simple_RNN_model.add(Dropout(0.2))
    # Final Layer
    simple_RNN_model.add(Dense(units=1))
    # Complie the model
    simple_RNN_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
    simple_RNN_model.summary()
    # Fit the model to train the model using Train data
    print("RUNNING SimpleRNN model on Training Data....")
    SimpleRNN_history = simple_RNN_model.fit(x_train_3D, y_train, epochs=100, batch_size=32)


    # #3rd model: GRU
    GRU_model = Sequential()
    # Layer 1
    GRU_model.add(GRU(units=50, return_sequences=True, input_shape=(x_train_3D.shape[1], 1)))
    GRU_model.add(Dropout(0.2))
    # Layer 2
    GRU_model.add(GRU(units=50, return_sequences=False))
    GRU_model.add(Dropout(0.2))
    # Final Layer
    GRU_model.add(Dense(units=1))
    # Compile the model
    GRU_model.compile(loss='mean_squared_error', optimizer='adam')
    GRU_model.summary()
    # Fit the model to train the model using Train data
    print("RUNNING GRU model on Training Data....")
    GRU_history = GRU_model.fit(x_train_3D, y_train, epochs=150, batch_size=32)

    # Save the models histroy to show in main program when we run it.
    print("SAVING the Histroy for all the three models")
    # Save the all the 3 models histroy as an npy file
    models_history = np.asarray([LSTM_history.history['loss'], SimpleRNN_history.history['loss'], GRU_history.history['loss']])
    #np.save('data/models_history.npy', models_history)
    dump(models_history, open('data/models_history.pkl', 'wb'))
    
    # Commenting ends here----------------------------
    '''
    # Required Commenting Ends ------------------------------------------------------------------------------------


    # We decided to use the LSTM model, Final model is as below
    print("We decided to use LSTM as regressor for further computation\n \n The model details are as below")

    # Load training data from train_data_RNN.csv
    train_dataset = pd.read_csv("data/train_data_RNN.csv")

    ## Carry our preprocessing and Prepare Data ##

    # Load the scaler
    minMax_data = load(open('data/minMax_data.pkl', 'rb'))
    minMax_label = load(open('data/minMax_label.pkl', 'rb'))

    # Transform the train data and labels
    x_train = minMax_data.transform(train_dataset.iloc[:, :-1])
    y_train = minMax_label.transform(train_dataset.iloc[:, -1:])

    # Convert the scaled data back to numpy array to be useful while generating the model
    x_train = np.array(x_train)
    x_train_3D = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))


    # Recurrent Neural network using LSTM as a regressor
    LSTM_model = Sequential()
    # Layer 1
    LSTM_model.add(LSTM(units=300, return_sequences=True, input_shape=(x_train_3D.shape[1], 1)))
    LSTM_model.add(Dropout(0.2))
    # Layer 2
    LSTM_model.add(LSTM(units=300, return_sequences=False))
    LSTM_model.add(Dropout(0.2))
    # Final Layer
    LSTM_model.add(Dense(units=1))

    # Compile LSTM model
    LSTM_model.compile(loss='mean_squared_error', optimizer='adam')
    LSTM_model.summary()

    # Fit the model to train the model using Train data
    print("RUNNING LSTM model on Training Data.....")
    LSTM_history = LSTM_model.fit(x_train_3D, y_train, epochs=500, batch_size=32)

    print("SAVING the LSTM model for further use")
    # Save the LSTM model
    LSTM_model.save('models/Group_44_RNN_model.h5')
    print("---> Model Saved\n\n")

    print("LOADING the history to plot the graph of Loss vs Epoch for Training data")
    # Load the models History to plot Loss vs epoch chart
    #histry = np.load('data/models_history.npy')
    histry = load(open('data/models_history.pkl', 'rb'))
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))

    ax1.plot(histry[0])
    ax2.plot(histry[1])
    ax3.plot(histry[2])

    ax1.set_title('LSTM model loss', fontweight="bold", size=20)
    ax1.legend(['training loss'], loc='upper right')
    ax1.set_ylabel('loss', size=15)
    ax1.set_xlabel('epoch', size=15)

    ax2.set_title('SimpleRNN model loss', fontweight="bold", size=20)
    ax2.legend(['training loss'], loc='upper right')
    ax2.set_ylabel('loss', size=15)
    ax2.set_xlabel('epoch', size=15)

    ax3.set_title('GRU model loss', fontweight="bold", size=20)
    ax3.legend(['training loss'], loc='upper right')
    ax3.set_ylabel('loss', size=15)
    ax3.set_xlabel('epoch', size=15)

    plt.tight_layout()
    plt.draw()


    print("LOADING saved model to run prediction for Training Dataset")
    # Load the Model
    RNN_model = load_model('models/Group_44_RNN_model.h5')
    print("----> Model Loaded successfully\n\n")

    # Evaluated the training model
    training_loss = RNN_model.evaluate(x_train_3D, y_train)
    print("\nTraining Loss is evaluated and is approximately --> ",training_loss)
    print("--------------------------------------------------------------------------")

    print("RUNNING predict on Training Data......")
    # Predict trained data
    train_predict = RNN_model.predict(x_train_3D)

    # Transform data back to original values from scaled values
    train_predict_unscaled = minMax_label.inverse_transform(train_predict)
    actual_train_label = minMax_label.inverse_transform(y_train)
    print("---> Train label Prediction complete \n \n")

    train_loss = MSE(actual_train_label, train_predict_unscaled)
    print("-------------------------------------------------------------")
    print("| The mean square error of TRAINING is: {}  |".format(train_loss))
    print("-------------------------------------------------------------")

    print("VISUALIZE Actual VS Predicted Opening Price chart")
    # Visualize Actual price vs Predicted price for Training dataset
    fig, ax = plt.subplots(figsize=(20, 5))
    plt.plot(actual_train_label, color='red', label='Actual Opening Price')
    ax.plot(train_predict_unscaled, color='blue', label='Predicted Opening Price')
    plt.title('Training data plot for Actual vs Predicted Opening Price')
    plt.ylabel('Price in $')
    plt.xlabel('Days')
    plt.legend()
    print("Ploting Please wait....")
    plt.draw()


    # Additonal analysis to compute with different past days
    # To look back 10 days in past n_past_days = 10
    print("\n\n Additional analysis if we increase number of past days\n We choose n_past_days = 30 to check past 30 days")
    stocks_30 = create_dataset(stock_df, n_past_days= 30)
    # Train test split
    x_30 = stocks_30.drop('Target', axis=1)
    y_30 = stocks_30.loc[:, 'Target':]

    # use sklearn train_test_split to randomize the data and split it into 70% train and 30% test data
    train_data, test_data, train_labels, test_labels = train_test_split(x_30, y_30, test_size=0.30, random_state=50)
    print("Size of train_data:   ", train_data.shape)
    print("Size of train_labels: ", train_labels.shape)
    print("Size of test_data:   ", test_data.shape)
    print("Size of test_labels: ", test_labels.shape)

    scaler_30 = MinMaxScaler(feature_range=(0, 1))
    # Transform the train data and labels
    x_train_30 = scaler_30.fit_transform(train_data)
    y_train_30 = scaler_30.fit_transform(train_labels)

    # Convert the scaled data back to numpy array to be useful while generating the model
    train_30 = np.array(x_train_30)
    train_30_3D = np.reshape(train_30, (train_30.shape[0], train_30.shape[1], 1))

    # Recurrent Neural network using LSTM as a regressor
    extra_model = Sequential()
    # Layer 1
    extra_model.add(LSTM(units=50, return_sequences=True, input_shape=(train_30_3D.shape[1], 1)))
    extra_model.add(Dropout(0.2))
    # Layer 2
    extra_model.add(LSTM(units=50, return_sequences=False))
    extra_model.add(Dropout(0.2))
    # Final Layer
    extra_model.add(Dense(units=1))

    # Compile LSTM model
    extra_model.compile(loss='mean_squared_error', optimizer='adam')
    extra_model.summary()

    print("---> Trained the model\n")
    trial_30_hist = extra_model.fit(train_30_3D, y_train_30, epochs=150, batch_size=32)

    print("RUNNING predict on Training Data......")
    # Predict trained data
    predict_30 = extra_model.predict(train_30_3D)
    # Transform data back to original values from scaled values
    predict_unscaled = minMax_label.inverse_transform(predict_30)
    actual_label = minMax_label.inverse_transform(y_train_30)
    print("---> Prediction complete \n \n")

    print("SAVING the Additional model for further use")
    # Save the LSTM model
    extra_model.save('models/Additional_RNN_model.h5')
    print("---> Model Saved")

    print("VISUALIZE Actual VS Predicted Opening Price chart")
    # Visualize Actual price vs Predicted price for Training dataset
    fig, ax = plt.subplots(figsize=(20, 5))
    plt.plot(actual_label, color='red', label='Actual Opening Price')
    ax.plot(predict_unscaled, color='blue', label='Predicted Opening Price')
    plt.title('Training data plot for Actual vs Predicted Opening Price no. of past day = 30')
    plt.ylabel('Price in $')
    plt.xlabel('Days')
    plt.legend()
    print("VISUALIZING Please wait....")
    plt.show()
